## Линейная регрессия

Линейная регрессия (Linear regression) — это математическая модель, которая описывает связь нескольких переменных.

$$
a(x) = \beta_0 + \beta_i d_i
$$

![img.png](img/img05.png)

### Матричная форма

$X$ - матрица выборки (объекты и выборки). Строчка - это объект, а столбик - признак, где $n$ - количество объектов в
выборке, а $k$ - количество признаков (включая константу).  
$B$ - это коэффициенты (веса).

Квадратичная функция качества:
$$
Q = \frac{1} {n} (X \cdot B - Y)^T(X \cdot B - Y)
$$

Минимум находится путем взятия матричного дифференциала:

$$
dQ(\beta^*) = 0
$$
или формула оптимальных коэффициентов для MSE:
$$
(X^T \cdot X)^{-1} \cdot X^T \cdot Y
$$

## Градиентный спуск (Gradient descent)

**Градиентный спуск** — метод нахождения локального минимума или максимума функции с помощью движения вдоль градиента.

**Градиент** — это вектор, своим направлением указывающий направление наибольшего возрастания некоторой скалярной
величины, а по модулю равный скорости роста этой величины в этом направлении. Тогда антиградиент напротив, показывает
наискорейшее убывание.

**Градиент функции** — это вектор, координатами которого являются частные производные этой функции по всем её
переменным.
Направление градиента есть направление наибыстрейшего возрастания функции, то есть производная функции (а точнее её
знак) показывает, в каком направлении функции возрастает.

$$
X_n = X_{n-1} - \eta \cdot f'(X_{n-1})
$$

$\eta$ - нормировачный коэффициент или темп обучения (learning rate)   
$\xi$ - критерий останова (threshold)

Алгоритм градиентного спуска (для функции одной переменной):

1. Инициализируемся в точке $X_{start}$
2. До сходимости:  
   $step = f'(X_{start})$  
   $X_{next} = X_{start} -\eta_i \cdot step$  
   $X_{start} = X_{next}$
3. Пока не сработает один из threshold:
    - Маленькое значение производной: $|f'(X_{n-1})| < \xi $
    - Маленький шаг: $|X_{n+1} - X_{n}| < \xi$
    - Маленькое изменение $f(X)$: $|f(X_{n+1}) - f(X_{n})| < \xi $

![img.png](img/img06.png)

Для функций нескольких переменных существует понятие линий уровня. _Линии уровня_ - множество точек, которые при
подстановке в функцию дают единое значение. Главное свойство линий уровня - они не пересекаются.

Алгоритм градиентного спуска (для функции нескольких переменных):

1. Инициализируемся в случайной точке $X_{start}$
2. До сходимости  
   $ step = \nabla z' (S_{start}) $  
   $ X_{next} = X_{start} -\eta_i \cdot step $  
   $ X_{start} = X_{next} $
3. Три варианта порога:
    - Маленькое значение производной: $|\nabla z(X_{n-1})| \leq \xi $
    - Маленький шаг: $|X_{n+1} - X_{n}| < \xi$
    - Маленькое изменение $f(X)$: $|f(X_{n+1}) - f(X_{n})| \leq \xi $

![img.png](img/img07.png)

### Общий алгоритм градиентного спуска 

Пусть имеем n объектов и m признаков:  
$$
\begin{aligned} &
Q = \frac {1} n \sum_{}^{n} (a(x_i) - y_i)^2 = \frac {1} n \sum_{i}^{n} (\beta_1 \cdot d_1^i +... +\beta_m \cdot d_m^i - y_i)^2 \\ &
Q'_{\beta_1} = \frac {2} n \sum_{i}^{n} d_1^i (\beta_1 \cdot d_1^i +... +\beta_m \cdot d_m^i - y_i) \\ &
... \\ &
Q'_{\beta_m} = \frac {2} n \sum_{i}^{n} d_m^i (\beta_1 \cdot d_1^i +... +\beta_m \cdot d_m^i - y_i) \\ &
\nabla Q = (Q'_{\beta_1} ... Q'_{\beta_m}) \\ &
X_{next} = X_{start} - \eta \cdot \nabla Q \\ &
\end{aligned}
$$

### Проблемы выбора нормировачного коэффициента $\eta$:

1. Большая длина шага приводит к перешагиванию желанной точки — глобального минимума и попадания вместо этого в
   локальный минимум.

2. Если большой, то градиент может взорваться - т.е. мы никогда не приблизимся к нему

3. Если слишком маленький, тогда критерий останова $\xi$ сработает раньше, чем мы окажемся в минимуме - затухание
   градиента

### Проблемы параметра $\xi$:

1. Консервативный спуск — маленький трешхолд позволит максимально приблизится к минимуму, но потребует много итераций.

2. Либеральный спуск — большой трешхолд позволит избежать большого количества итераций, однако может привести к раннему
   срабатыванию критерия останова.


